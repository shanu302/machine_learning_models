{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data_sets/bank-additional-full.csv'\n",
    "\n",
    "data = pd.read_csv(path, sep = ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the categorical variables\n",
    "categs = ['job','marital','education','default','housing','loan','contact','month','day_of_week','duration','poutcome','y']\n",
    "\n",
    "\n",
    "#now we will extract quantitive vars\n",
    "\n",
    "quantit = [i for i in var_names if i not in categs]\n",
    "\n",
    "# Convert categorical variable into dummy/indicator variables\n",
    "\n",
    "job = pd.get_dummies(data['job'])\n",
    "marital = pd.get_dummies(data['marital'])\n",
    "education = pd.get_dummies(data['education'])\n",
    "default = pd.get_dummies(data['default'])\n",
    "housing = pd.get_dummies(data['housing'])\n",
    "loan = pd.get_dummies(data['loan'])\n",
    "contact = pd.get_dummies(data['contact'])\n",
    "month = pd.get_dummies(data['month'])\n",
    "day_of_week = pd.get_dummies(data['day_of_week'])\n",
    "duration = pd.get_dummies(data['duration'])\n",
    "poutcome = pd.get_dummies(data['poutcome'])\n",
    "\n",
    "# now y column still has value yes and no\n",
    "#so convert it t 1 and 0\n",
    "\n",
    "dict_map = dict()\n",
    "y_map = {'yes': 1,'no':0}\n",
    "dict_map['y'] = y_map\n",
    "\n",
    "#now replace the y column in data with the new y \n",
    "\n",
    "data = data.replace(dict_map)\n",
    "label = data['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data[quantit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data_columns = numerical_data.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using minmax scaler \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "#convert scaled data to data frame object\n",
    "\n",
    "scaled_data_data_frame = pd.DataFrame(scaled_data)\n",
    "#assign name to colums of new data frmae object created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_data_frame.columns = numerical_data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now concatenate all data aka categorical datat and numerical data\n",
    "\n",
    "normalized_data = pd.concat([scaled_data_data_frame,job,marital,education,default,housing,loan,contact,month,\n",
    "                            day_of_week,duration,poutcome,label],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the data to csv file \n",
    "\n",
    "normalized_data.to_csv('bank-normalized.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the nueral network for the above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all define all hyper-parameters\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUMBER_EPOCH = 100\n",
    "BATCH_SIZE = 100\n",
    "DISPLAY_EPOCH = 20\n",
    "N_HIDDEN_NEURON = 250\n",
    "STDDEV = 0.1\n",
    "ACTIVATION_FUNCTION_OUT = tf.nn.tanh\n",
    "RANDOM_STATE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now ready the data_set we have prepared earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA LOADED SUFFESSFULLY\n"
     ]
    }
   ],
   "source": [
    "doc_location = os.getcwd() + '/bank_normalized.csv'\n",
    "read_data = pd.read_csv(doc_location)\n",
    "print(\"RAW DATA LOADED SUFFESSFULLY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will define data specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_LABEL = 'y'\n",
    "\n",
    "DATA_COLUMNS = [i for i in read_data.columns.tolist() if i != Y_LABEL]\n",
    "\n",
    "M = read_data.shape[0]  # USUAL NOTATION FOR TOTAL NUMBER OF TRAINING EXAMPLES\n",
    "\n",
    "N = read_data.shape[1] - 1 # total number of features -1 to subtract y label \n",
    "\n",
    "N_CLASSES = read_data[Y_LABEL].unique().shape[0]\n",
    "\n",
    "TEST_SIZE = 0.25\n",
    "TRAIN_SIZE = int(M*(1 - TEST_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW WE LOAD X DATA AND Y DATAT\n",
    "x_data = read_data[DATA_COLUMNS].get_values()\n",
    "y_data = read_data[Y_LABEL].get_values()\n",
    "\n",
    "#one hot encoding for y_data\n",
    "labels_ = np.zeros((M, N_CLASSES))\n",
    "labels_[np.arange(M), y_data] = 1\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,labels_,\n",
    "                                                 test_size = TEST_SIZE,\n",
    "                                                 random_state = RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = N\n",
    "n_hidden_1 = N_HIDDEN_NEURON\n",
    "n_hidden_2 = N_HIDDEN_NEURON\n",
    "n_hidden_3 = N_HIDDEN_NEURON\n",
    "n_hidden_4 = N_HIDDEN_NEURON\n",
    "n_classes = N_CLASSES\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None,n_input])\n",
    "y = tf.placeholder(tf.float32, shape = [None, n_classes])\n",
    "\n",
    "def mpl(_X,_weights,_biases,dropout_keep_prob):\n",
    "    layer1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(_X,_weights['w1']), _biases['b1'])),dropout_keep_prob)\n",
    "    layer2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer1,_weights['w2']), _biases['b2'])),dropout_keep_prob)\n",
    "    layer3 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer2,_weights['w3']), _biases['b3'])),dropout_keep_prob)\n",
    "    layer4 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer3,_weights['w4']), _biases['b4'])),dropout_keep_prob)\n",
    "    out = ACTIVATION_FUNCTION_OUT((tf.add(tf.matmul(layer4,_weights['w5']), _biases['b5'])))\n",
    "    return out\n",
    "\n",
    "weights = {\n",
    "    'w1' : tf.Variable(tf.random_normal([n_input,n_hidden_1],stddev = STDDEV)),\n",
    "    'w2' : tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2],stddev = STDDEV)),\n",
    "    'w3' : tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3],stddev = STDDEV)),\n",
    "    'w4' : tf.Variable(tf.random_normal([n_hidden_3,n_hidden_4],stddev = STDDEV)),\n",
    "    'w5' : tf.Variable(tf.random_normal([n_hidden_4,n_classes],stddev = STDDEV))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hidden_1],stddev = STDDEV)),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hidden_2],stddev = STDDEV)),\n",
    "    'b3' : tf.Variable(tf.random_normal([n_hidden_3],stddev = STDDEV)),\n",
    "    'b4' : tf.Variable(tf.random_normal([n_hidden_4],stddev = STDDEV)),\n",
    "    'b5' : tf.Variable(tf.random_normal([n_classes],stddev = STDDEV))\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the mathematical calculateions beware\n",
    "predict = mpl(X,weights,biases,dropout_keep_prob)\n",
    "\n",
    "cost_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = predict, labels = y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP NEURAL NETWORD HAS BEEN BUILT SUCCESSFULLY\n",
      "STARTED TRAINING\n"
     ]
    }
   ],
   "source": [
    "# now compute the accuracy of \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(\"DEEP NEURAL NETWORD HAS BEEN BUILT SUCCESSFULLY\")\n",
    "print(\"STARTED TRAINING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/100 cost : 0.214328094\n",
      "training accuracy 0.980\n",
      "Epoch 020/100 cost : 0.209041875\n",
      "training accuracy 0.970\n",
      "Epoch 040/100 cost : 0.208228864\n",
      "training accuracy 0.960\n",
      "Epoch 060/100 cost : 0.208879270\n",
      "training accuracy 0.970\n",
      "Epoch 080/100 cost : 0.216196346\n",
      "training accuracy 0.970\n",
      "End of training.\n",
      "\n",
      "Testing....\n",
      "\n",
      "Test accuracy: 0.956\n",
      "Your mlp model has beel successfuly trained\n"
     ]
    }
   ],
   "source": [
    "#NOW THE TYPICAL TENSORFLOWS JOB\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    # this one is the traningloop\n",
    "    for i in range(NUMBER_EPOCH):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(x_train.shape[0]//BATCH_SIZE)\n",
    "        \n",
    "        # this one is to loop over all batches\n",
    "        for j in range(total_batch):\n",
    "            rand_id_x = np.random.randint(int(TRAIN_SIZE), size = BATCH_SIZE)\n",
    "            batch_xs = x_train[rand_id_x, :]\n",
    "            batch_ys = y_train[rand_id_x,:]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #now fit this randomly indexes batched data\n",
    "            sess.run(optimizer, feed_dict={X:batch_xs,y:batch_ys,dropout_keep_prob:0.9})\n",
    "            \n",
    "            avg_cost+= sess.run(cost_op,feed_dict={X:batch_xs,y:batch_ys,dropout_keep_prob:0.9})/total_batch\n",
    "            \n",
    "        if i % DISPLAY_EPOCH ==0:\n",
    "            print(\"Epoch %03d/%03d cost : %.9f\" % (i,NUMBER_EPOCH,avg_cost))\n",
    "            train_acc = sess.run(accuracy,feed_dict={X:batch_xs, y:batch_ys, dropout_keep_prob:1.})\n",
    "            print(\"training accuracy %.3f\" % (train_acc))\n",
    "    print(\"End of training.\\n\")\n",
    "    print(\"Testing....\\n\")\n",
    "    \n",
    "    #now test the data\n",
    "    \n",
    "    test_acc = sess.run(accuracy,feed_dict={X:x_test, y: y_test,dropout_keep_prob:1.})\n",
    "    print(\"Test accuracy: %.3f\" % (test_acc))\n",
    "    print(\"Your mlp model has beel successfuly trained\")\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
